#!/usr/bin/env python3
"""
ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ v1.0
ì‹œìŠ¤í…œ ì‘ë‹µ ì†ë„, íŒŒì¼ ì²˜ë¦¬ ì†ë„, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
"""
import os
import time
import psutil
import subprocess
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import json

class PerformanceOptimizer:
    def __init__(self):
        self.root = Path("C:/Users/eunta/multi-agent-workspace")
        self.metrics = {}
        
    def measure_system_performance(self):
        """ì‹œìŠ¤í…œ ì„±ëŠ¥ ì¸¡ì •"""
        print("ğŸ“Š ì‹œìŠ¤í…œ ì„±ëŠ¥ ì¸¡ì • ì¤‘...")
        
        # CPU ì‚¬ìš©ë¥ 
        cpu_percent = psutil.cpu_percent(interval=1)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
        memory = psutil.virtual_memory()
        memory_percent = memory.percent
        memory_available = memory.available / (1024**3)  # GB
        
        # ë””ìŠ¤í¬ I/O
        disk_usage = psutil.disk_usage('C:/')
        disk_free = disk_usage.free / (1024**3)  # GB
        
        self.metrics = {
            'cpu_percent': cpu_percent,
            'memory_percent': memory_percent,
            'memory_available_gb': round(memory_available, 1),
            'disk_free_gb': round(disk_free, 1),
            'timestamp': time.time()
        }
        
        print(f"1) CPU ì‚¬ìš©ë¥ : {cpu_percent}%")
        print(f"2) ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {memory_percent}%")
        print(f"3) ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: {self.metrics['memory_available_gb']}GB")
        print(f"4) ë””ìŠ¤í¬ ì—¬ìœ  ê³µê°„: {self.metrics['disk_free_gb']}GB")
        
        return self.metrics
    
    def optimize_python_performance(self):
        """Python ì„±ëŠ¥ ìµœì í™”"""
        print("ğŸ Python ì„±ëŠ¥ ìµœì í™” ì¤‘...")
        optimizations = []
        
        # 1. ë°”ì´íŠ¸ì½”ë“œ ìµœì í™”
        print("1) ë°”ì´íŠ¸ì½”ë“œ ì»´íŒŒì¼ ìµœì í™”...")
        python_files = list(self.root.rglob("*.py"))
        
        def compile_file(py_file):
            try:
                import py_compile
                py_compile.compile(py_file, doraise=True)
                return True
            except:
                return False
        
        with ThreadPoolExecutor(max_workers=4) as executor:
            results = list(executor.map(compile_file, python_files[:50]))  # ì²˜ìŒ 50ê°œë§Œ
        
        compiled_count = sum(results)
        optimizations.append(f"Python íŒŒì¼ {compiled_count}ê°œ ë°”ì´íŠ¸ì½”ë“œ ìµœì í™”")
        
        # 2. Import ìµœì í™”
        print("2) Import ìµœì í™” ë¶„ì„...")
        import_analysis = self.analyze_imports()
        if import_analysis['unused_imports'] > 0:
            optimizations.append(f"ë¶ˆí•„ìš”í•œ import {import_analysis['unused_imports']}ê°œ ë°œê²¬")
        
        # 3. ë©”ëª¨ë¦¬ ì •ë¦¬
        print("3) ë©”ëª¨ë¦¬ ì •ë¦¬...")
        import gc
        before_objects = len(gc.get_objects())
        collected = gc.collect()
        after_objects = len(gc.get_objects())
        
        if collected > 0:
            optimizations.append(f"ê°€ë¹„ì§€ ì»¬ë ‰ì…˜: {collected}ê°œ ê°ì²´ ì •ë¦¬")
        
        return optimizations
    
    def analyze_imports(self):
        """Import ì‚¬ìš© ë¶„ì„"""
        analysis = {
            'total_files': 0,
            'total_imports': 0,
            'unused_imports': 0,
            'duplicate_imports': 0
        }
        
        for py_file in self.root.rglob("*.py"):
            if 'venv' in str(py_file) or '.git' in str(py_file):
                continue
                
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                analysis['total_files'] += 1
                
                # ê°„ë‹¨í•œ import ì¹´ìš´íŠ¸ (ì •í™•í•˜ì§€ ì•Šì§€ë§Œ ê°œëµì  íŒŒì•…)
                import_lines = [line for line in content.split('\n') 
                               if line.strip().startswith(('import ', 'from '))]
                analysis['total_imports'] += len(import_lines)
                
                # ì¤‘ë³µ import ê°ì§€
                unique_imports = set(import_lines)
                if len(import_lines) > len(unique_imports):
                    analysis['duplicate_imports'] += len(import_lines) - len(unique_imports)
                    
            except:
                continue
        
        return analysis
    
    def optimize_file_operations(self):
        """íŒŒì¼ ì‘ì—… ìµœì í™”"""
        print("ğŸ“ íŒŒì¼ ì‘ì—… ìµœì í™” ì¤‘...")
        optimizations = []
        
        # 1. ì„ì‹œ íŒŒì¼ ì •ë¦¬
        temp_files = []
        for pattern in ['*.tmp', '*.temp', '*~', '*.bak']:
            temp_files.extend(self.root.rglob(pattern))
        
        if temp_files:
            for temp_file in temp_files[:10]:  # ì•ˆì „í•˜ê²Œ ì²˜ìŒ 10ê°œë§Œ
                try:
                    temp_file.unlink()
                except:
                    pass
            optimizations.append(f"ì„ì‹œ íŒŒì¼ {len(temp_files)}ê°œ ì •ë¦¬")
        
        # 2. ë¹ˆ í´ë” ì •ë¦¬
        empty_dirs = []
        for dir_path in self.root.rglob("*"):
            if dir_path.is_dir() and not any(dir_path.iterdir()):
                if 'venv' not in str(dir_path) and '.git' not in str(dir_path):
                    empty_dirs.append(dir_path)
        
        for empty_dir in empty_dirs[:5]:  # ì•ˆì „í•˜ê²Œ ì²˜ìŒ 5ê°œë§Œ
            try:
                empty_dir.rmdir()
            except:
                pass
        
        if empty_dirs:
            optimizations.append(f"ë¹ˆ í´ë” {len(empty_dirs)}ê°œ ì •ë¦¬")
        
        # 3. ìºì‹œ ë””ë ‰í† ë¦¬ í¬ê¸° í™•ì¸
        cache_dirs = [
            self.root / "__pycache__",
            self.root / ".pytest_cache",
            self.root / ".agents" / "cache"
        ]
        
        total_cache_size = 0
        for cache_dir in cache_dirs:
            if cache_dir.exists():
                for file in cache_dir.rglob("*"):
                    if file.is_file():
                        total_cache_size += file.stat().st_size
        
        cache_size_mb = total_cache_size / (1024 * 1024)
        if cache_size_mb > 100:  # 100MB ì´ìƒ
            optimizations.append(f"ìºì‹œ í¬ê¸° {cache_size_mb:.1f}MB - ì •ë¦¬ ê¶Œì¥")
        
        return optimizations
    
    def optimize_database_performance(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ìµœì í™”"""
        print("ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ì¤‘...")
        optimizations = []
        
        # SQLite ë°ì´í„°ë² ì´ìŠ¤ ì°¾ê¸°
        db_files = list(self.root.glob("*.db"))
        
        for db_file in db_files:
            try:
                import sqlite3
                conn = sqlite3.connect(str(db_file))
                
                # VACUUM ì‹¤í–‰ (ë°ì´í„°ë² ì´ìŠ¤ ì••ì¶•)
                conn.execute("VACUUM")
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                conn.execute("ANALYZE")
                
                conn.close()
                optimizations.append(f"{db_file.name} ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”")
                
            except:
                continue
        
        return optimizations
    
    def generate_performance_report(self):
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = {
            'timestamp': time.time(),
            'metrics': self.metrics,
            'optimizations_applied': [],
            'recommendations': []
        }
        
        # ì„±ëŠ¥ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if self.metrics.get('memory_percent', 0) > 85:
            report['recommendations'].append("ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë†’ìŒ - ë¶ˆí•„ìš”í•œ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ê¶Œì¥")
        
        if self.metrics.get('cpu_percent', 0) > 80:
            report['recommendations'].append("CPU ì‚¬ìš©ë¥  ë†’ìŒ - ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… í™•ì¸ ê¶Œì¥")
        
        if self.metrics.get('disk_free_gb', 0) < 5:
            report['recommendations'].append("ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡± - íŒŒì¼ ì •ë¦¬ í•„ìš”")
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        report_file = self.root / "reports" / f"performance_report_{int(time.time())}.json"
        report_file.parent.mkdir(exist_ok=True)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2)
        
        return report, report_file
    
    def run_full_optimization(self):
        """ì „ì²´ ìµœì í™” ì‹¤í–‰"""
        print("ğŸš€ ì„±ëŠ¥ ìµœì í™” ì‹œì‘...")
        print("=" * 50)
        
        start_time = time.time()
        all_optimizations = []
        
        # 1. ì‹œìŠ¤í…œ ì„±ëŠ¥ ì¸¡ì •
        self.measure_system_performance()
        print()
        
        # 2. Python ì„±ëŠ¥ ìµœì í™”
        python_opts = self.optimize_python_performance()
        all_optimizations.extend(python_opts)
        print()
        
        # 3. íŒŒì¼ ì‘ì—… ìµœì í™”
        file_opts = self.optimize_file_operations()
        all_optimizations.extend(file_opts)
        print()
        
        # 4. ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”
        db_opts = self.optimize_database_performance()
        all_optimizations.extend(db_opts)
        print()
        
        # 5. ê²°ê³¼ ë¦¬í¬íŠ¸
        report, report_file = self.generate_performance_report()
        report['optimizations_applied'] = all_optimizations
        
        # ë¦¬í¬íŠ¸ ì¬ì €ì¥
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        end_time = time.time()
        optimization_time = end_time - start_time
        
        print("=" * 50)
        print("ğŸ‰ ì„±ëŠ¥ ìµœì í™” ì™„ë£Œ!")
        print(f"ì†Œìš” ì‹œê°„: {optimization_time:.1f}ì´ˆ")
        print(f"ì ìš©ëœ ìµœì í™”: {len(all_optimizations)}ê°œ")
        
        for i, opt in enumerate(all_optimizations, 1):
            print(f"  {i}) {opt}")
        
        if report['recommendations']:
            print("\nğŸ’¡ ì¶”ê°€ ê¶Œì¥ì‚¬í•­:")
            for i, rec in enumerate(report['recommendations'], 1):
                print(f"  {i}) {rec}")
        
        print(f"\nğŸ“„ ìƒì„¸ ë¦¬í¬íŠ¸: {report_file}")
        
        return report

def main():
    optimizer = PerformanceOptimizer()
    optimizer.run_full_optimization()

if __name__ == "__main__":
    main()