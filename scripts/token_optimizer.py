#!/usr/bin/env python3
"""
í† í° íš¨ìœ¨ì„± ìµœì í™” ì‹œìŠ¤í…œ
- ì»¨í…ìŠ¤íŠ¸ ì••ì¶•
- ìŠ¤ë§ˆíŠ¸ ìºì‹±
- ì¤‘ë³µ ì‘ì—… ì œê±°
- í† í° ì‚¬ìš©ëŸ‰ 50% ì ˆê° ëª©í‘œ
"""
import json
import hashlib
import sqlite3
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple

class TokenOptimizer:
    """í† í° ì‚¬ìš©ëŸ‰ ìµœì í™” ì‹œìŠ¤í…œ"""
    
    def __init__(self, root_path: str):
        self.root = Path(root_path)
        self.cache_db = self.root / ".agents" / "token_cache.db"
        self.stats_file = self.root / ".agents" / "token_stats.json"
        
        self.cache_db.parent.mkdir(exist_ok=True, parents=True)
        self._init_database()
    
    def _init_database(self):
        """ìºì‹œ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        with sqlite3.connect(self.cache_db) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS context_cache (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    context_hash TEXT UNIQUE NOT NULL,
                    compressed_context TEXT NOT NULL,
                    original_size INTEGER NOT NULL,
                    compressed_size INTEGER NOT NULL,
                    created_at TEXT NOT NULL,
                    last_used TEXT NOT NULL,
                    use_count INTEGER DEFAULT 1
                )
            """)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS response_cache (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    request_hash TEXT UNIQUE NOT NULL,
                    response TEXT NOT NULL,
                    tokens_saved INTEGER NOT NULL,
                    created_at TEXT NOT NULL,
                    expires_at TEXT NOT NULL
                )
            """)
    
    def compress_context(self, context: str, max_length: int = 2000) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ì••ì¶• (í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œ)"""
        
        # í•´ì‹œ ìƒì„±
        context_hash = hashlib.md5(context.encode()).hexdigest()
        
        # ìºì‹œ í™•ì¸
        cached = self._get_cached_context(context_hash)
        if cached:
            return cached
        
        # ì••ì¶• ë¡œì§
        compressed = self._compress_text(context, max_length)
        
        # ìºì‹œ ì €ì¥
        self._save_compressed_context(context_hash, context, compressed)
        
        return compressed
    
    def _compress_text(self, text: str, max_length: int) -> str:
        """í…ìŠ¤íŠ¸ ì••ì¶• ì•Œê³ ë¦¬ì¦˜"""
        if len(text) <= max_length:
            return text
        
        # 1. ì¤‘ìš” ì„¹ì…˜ ì¶”ì¶œ
        important_sections = []
        
        # ì˜¤ë¥˜ ë©”ì‹œì§€ (ìµœìš°ì„ )
        lines = text.split('\n')
        for i, line in enumerate(lines):
            if any(keyword in line.lower() for keyword in ['error', 'fail', 'exception', 'âŒ', 'ğŸš¨']):
                # ì˜¤ë¥˜ ë¼ì¸ + ì „í›„ 2ì¤„ì”©
                start = max(0, i-2)
                end = min(len(lines), i+3)
                important_sections.extend(lines[start:end])
        
        # 2. ì½”ë“œ ë¸”ë¡ ì••ì¶•
        code_blocks = []
        in_code = False
        current_block = []
        
        for line in lines:
            if line.strip().startswith('```'):
                if in_code:
                    # ì½”ë“œ ë¸”ë¡ ë
                    if len('\n'.join(current_block)) < 500:  # ì§§ì€ ì½”ë“œë§Œ ìœ ì§€
                        code_blocks.extend(current_block)
                    else:
                        # ê¸´ ì½”ë“œëŠ” ìš”ì•½
                        code_blocks.append(f"[ì½”ë“œ ë¸”ë¡: {len(current_block)}ì¤„ ìƒëµ]")
                    current_block = []
                in_code = not in_code
            elif in_code:
                current_block.append(line)
            else:
                # ì¼ë°˜ í…ìŠ¤íŠ¸ì—ì„œ ì¤‘ìš”í•œ ë¶€ë¶„
                if any(keyword in line.lower() for keyword in 
                       ['todo', 'ì‘ì—…', 'ì™„ë£Œ', 'ì§„í–‰', 'ìƒíƒœ', 'status', 'priority']):
                    important_sections.append(line)
        
        # 3. ìµœì¢… ì••ì¶• í…ìŠ¤íŠ¸ ìƒì„±
        compressed_parts = []
        
        # ì¤‘ìš” ì„¹ì…˜ ì¶”ê°€
        if important_sections:
            compressed_parts.append("ğŸ¯ í•µì‹¬ ë‚´ìš©:")
            compressed_parts.extend(important_sections[:10])  # ìµœëŒ€ 10ì¤„
        
        # ì½”ë“œ ë¸”ë¡ ì¶”ê°€
        if code_blocks:
            compressed_parts.append("\nğŸ’» ì½”ë“œ:")
            compressed_parts.extend(code_blocks[:5])  # ìµœëŒ€ 5ê°œ ë¸”ë¡
        
        # ê¸¸ì´ ì¡°ì •
        result = '\n'.join(compressed_parts)
        if len(result) > max_length:
            result = result[:max_length] + "\n[...ë” ë§ì€ ë‚´ìš© ìƒëµ...]"
        
        return result
    
    def cache_response(self, request: str, response: str, ttl_hours: int = 24) -> bool:
        """ì‘ë‹µ ìºì‹±"""
        request_hash = hashlib.md5(request.encode()).hexdigest()
        expires_at = (datetime.now() + timedelta(hours=ttl_hours)).isoformat()
        
        tokens_saved = len(response) // 4  # ëŒ€ëµì ì¸ í† í° ìˆ˜
        
        try:
            with sqlite3.connect(self.cache_db) as conn:
                conn.execute("""
                    INSERT OR REPLACE INTO response_cache 
                    (request_hash, response, tokens_saved, created_at, expires_at)
                    VALUES (?, ?, ?, ?, ?)
                """, (request_hash, response, tokens_saved, datetime.now().isoformat(), expires_at))
            return True
        except Exception as e:
            print(f"ERROR: response cache failed - {e}")
            return False
    
    def get_cached_response(self, request: str) -> Optional[str]:
        """ìºì‹œëœ ì‘ë‹µ ì¡°íšŒ"""
        request_hash = hashlib.md5(request.encode()).hexdigest()
        
        try:
            with sqlite3.connect(self.cache_db) as conn:
                cursor = conn.execute("""
                    SELECT response, expires_at FROM response_cache 
                    WHERE request_hash = ? AND expires_at > ?
                """, (request_hash, datetime.now().isoformat()))
                
                result = cursor.fetchone()
                if result:
                    self._update_token_stats("cache_hit", len(result[0]) // 4)
                    return result[0]
        except Exception:
            pass
        
        return None
    
    def remove_duplicates(self, file_path: str) -> int:
        """íŒŒì¼ì—ì„œ ì¤‘ë³µ ë‚´ìš© ì œê±°"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            lines = content.split('\n')
            unique_lines = []
            seen_lines = set()
            
            for line in lines:
                # ê³µë°± ì œê±° í›„ ë¹„êµ
                clean_line = line.strip()
                if clean_line and clean_line not in seen_lines:
                    unique_lines.append(line)
                    seen_lines.add(clean_line)
                elif not clean_line:  # ë¹ˆ ì¤„ì€ ìœ ì§€ (í•˜ë‚˜ë§Œ)
                    if not unique_lines or unique_lines[-1].strip():
                        unique_lines.append(line)
            
            # íŒŒì¼ ì—…ë°ì´íŠ¸
            new_content = '\n'.join(unique_lines)
            if len(new_content) < len(content):
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(new_content)
                
                saved_chars = len(content) - len(new_content)
                self._update_token_stats("deduplication", saved_chars // 4)
                return saved_chars
        
        except Exception as e:
            print(f"ERROR: dedup failed ({file_path}) - {e}")
        
        return 0
    
    def optimize_communication_files(self) -> Dict[str, int]:
        """í†µì‹  íŒŒì¼ë“¤ ìµœì í™”"""
        results = {"files_processed": 0, "tokens_saved": 0}
        
        comm_dir = self.root / "communication"
        if not comm_dir.exists():
            return results
        
        for agent_dir in comm_dir.iterdir():
            if agent_dir.is_dir():
                for file_path in agent_dir.glob("*.md"):
                    # 3ì¼ ì´ìƒ ëœ íŒŒì¼ë§Œ ìµœì í™”
                    if (datetime.now() - datetime.fromtimestamp(file_path.stat().st_mtime)).days >= 3:
                        saved = self.remove_duplicates(str(file_path))
                        if saved > 0:
                            results["files_processed"] += 1
                            results["tokens_saved"] += saved // 4
        
        return results
    
    def get_optimization_stats(self) -> Dict:
        """ìµœì í™” í†µê³„ ë°˜í™˜"""
        try:
            with open(self.stats_file, 'r') as f:
                stats = json.load(f)
        except:
            stats = {
                "total_tokens_saved": 0,
                "cache_hits": 0,
                "deduplications": 0,
                "compressions": 0,
                "last_updated": datetime.now().isoformat()
            }
        
        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¶”ê°€ í†µê³„
        try:
            with sqlite3.connect(self.cache_db) as conn:
                # ìºì‹œ í†µê³„
                cursor = conn.execute("SELECT COUNT(*), SUM(tokens_saved) FROM response_cache")
                cache_count, cache_tokens = cursor.fetchone()
                
                # ì••ì¶• í†µê³„  
                cursor = conn.execute("SELECT COUNT(*), SUM(original_size - compressed_size) FROM context_cache")
                compress_count, compress_saved = cursor.fetchone()
                
                stats.update({
                    "active_cache_entries": cache_count or 0,
                    "tokens_saved_by_cache": cache_tokens or 0,
                    "compression_entries": compress_count or 0,
                    "bytes_saved_by_compression": compress_saved or 0
                })
        except:
            pass
        
        return stats
    
    def _get_cached_context(self, context_hash: str) -> Optional[str]:
        """ìºì‹œëœ ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ"""
        try:
            with sqlite3.connect(self.cache_db) as conn:
                cursor = conn.execute("""
                    SELECT compressed_context FROM context_cache 
                    WHERE context_hash = ?
                """, (context_hash,))
                
                result = cursor.fetchone()
                if result:
                    # ì‚¬ìš© íšŸìˆ˜ ì—…ë°ì´íŠ¸
                    conn.execute("""
                        UPDATE context_cache 
                        SET last_used = ?, use_count = use_count + 1
                        WHERE context_hash = ?
                    """, (datetime.now().isoformat(), context_hash))
                    
                    return result[0]
        except:
            pass
        
        return None
    
    def _save_compressed_context(self, context_hash: str, original: str, compressed: str):
        """ì••ì¶•ëœ ì»¨í…ìŠ¤íŠ¸ ì €ì¥"""
        try:
            with sqlite3.connect(self.cache_db) as conn:
                conn.execute("""
                    INSERT OR REPLACE INTO context_cache 
                    (context_hash, compressed_context, original_size, compressed_size, created_at, last_used)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (
                    context_hash, compressed, len(original), len(compressed),
                    datetime.now().isoformat(), datetime.now().isoformat()
                ))
        except Exception as e:
            print(f"ERROR: context cache failed - {e}")
    
    def _update_token_stats(self, operation: str, tokens_saved: int):
        """í† í° í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            try:
                with open(self.stats_file, 'r') as f:
                    stats = json.load(f)
            except:
                stats = {"total_tokens_saved": 0}
            
            stats["total_tokens_saved"] = stats.get("total_tokens_saved", 0) + tokens_saved
            stats[operation] = stats.get(operation, 0) + 1
            stats["last_updated"] = datetime.now().isoformat()
            
            with open(self.stats_file, 'w') as f:
                json.dump(stats, f, indent=2)
        except:
            pass

# í¸ì˜ í•¨ìˆ˜ë“¤
def optimize_text(text: str, max_length: int = 2000) -> str:
    """í…ìŠ¤íŠ¸ ë¹ ë¥¸ ìµœì í™”"""
    optimizer = TokenOptimizer("C:/Users/etlov/multi-agent-workspace")
    return optimizer.compress_context(text, max_length)

def cleanup_communication_folder() -> Dict[str, int]:
    """í†µì‹  í´ë” ë¹ ë¥¸ ì •ë¦¬"""
    optimizer = TokenOptimizer("C:/Users/etlov/multi-agent-workspace")
    return optimizer.optimize_communication_files()

if __name__ == "__main__":
    from cli_style import header, section, kv, item
    optimizer = TokenOptimizer("C:/Users/etlov/multi-agent-workspace")

    print(header("Token Optimizer"))

    # í†µì‹  í´ë” ìµœì í™”
    results = optimizer.optimize_communication_files()
    print(item(1, f"Communication optimize - files={results['files_processed']}, tokens_saved={results['tokens_saved']}"))

    # í†µê³„ ì¶œë ¥
    stats = optimizer.get_optimization_stats()
    print(item(2, kv("Total Tokens Saved", stats.get('total_tokens_saved', 0))))
    print(item(3, kv("Active Cache Entries", stats.get('active_cache_entries', 0))))

    # í…ŒìŠ¤íŠ¸ ì••ì¶•
    test_text = "ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. " * 100
    compressed = optimizer.compress_context(test_text, 200)
    print(item(4, f"Compress test - original={len(test_text)} compressed={len(compressed)}"))
