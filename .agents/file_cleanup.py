#!/usr/bin/env python3
"""
ë¶ˆí•„ìš”í•œ íŒŒì¼ ì •ë¦¬ ë° ìµœì í™” ë„êµ¬
- ì„ì‹œ íŒŒì¼, ìºì‹œ íŒŒì¼, ì¤‘ë³µ íŒŒì¼ ì œê±°
- ë¡œê·¸ íŒŒì¼ ì •ë¦¬
- ë””ìŠ¤í¬ ê³µê°„ ìµœì í™”
"""

import os
import shutil
import re
from pathlib import Path
from datetime import datetime, timedelta
import json
import hashlib

class FileCleanup:
    def __init__(self, workspace_path="."):
        self.workspace = Path(workspace_path)
        self.cleaned_files = []
        self.saved_space = 0
        
    def find_temp_files(self):
        """ì„ì‹œ íŒŒì¼ ì°¾ê¸°"""
        temp_patterns = [
            "*.tmp", "*.temp", "*.swp", "*.swo",
            "*~", "*.bak", "*.orig", "*.cache",
            "thumbs.db", ".DS_Store", "desktop.ini"
        ]
        
        temp_files = []
        for pattern in temp_patterns:
            temp_files.extend(self.workspace.rglob(pattern))
        
        return temp_files
    
    def find_duplicate_files(self):
        """ì¤‘ë³µ íŒŒì¼ ì°¾ê¸° (í•´ì‹œ ê¸°ë°˜)"""
        file_hashes = {}
        duplicates = []
        
        # Python, í…ìŠ¤íŠ¸ íŒŒì¼ë§Œ ê²€ì‚¬
        for file_path in self.workspace.rglob("*"):
            if (file_path.is_file() and 
                file_path.suffix in ['.py', '.txt', '.md', '.json'] and
                file_path.stat().st_size > 100):  # 100ë°”ì´íŠ¸ ì´ìƒë§Œ
                
                try:
                    with open(file_path, 'rb') as f:
                        file_hash = hashlib.md5(f.read()).hexdigest()
                    
                    if file_hash in file_hashes:
                        duplicates.append((file_path, file_hashes[file_hash]))
                    else:
                        file_hashes[file_hash] = file_path
                except:
                    continue
        
        return duplicates
    
    def find_old_logs(self, days=30):
        """ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ ì°¾ê¸°"""
        cutoff_date = datetime.now() - timedelta(days=days)
        old_logs = []
        
        log_dirs = [
            self.workspace / "logs",
            self.workspace / "terminal_logs", 
            self.workspace / ".agents" / "backup"
        ]
        
        for log_dir in log_dirs:
            if log_dir.exists():
                for log_file in log_dir.rglob("*"):
                    if (log_file.is_file() and 
                        datetime.fromtimestamp(log_file.stat().st_mtime) < cutoff_date):
                        old_logs.append(log_file)
        
        return old_logs
    
    def find_empty_directories(self):
        """ë¹ˆ ë””ë ‰í† ë¦¬ ì°¾ê¸°"""
        empty_dirs = []
        
        for dir_path in self.workspace.rglob("*"):
            if (dir_path.is_dir() and 
                not any(dir_path.iterdir()) and
                dir_path.name not in ['.git', '.vscode', '.agents']):
                empty_dirs.append(dir_path)
        
        return empty_dirs
    
    def find_large_files(self, size_mb=10):
        """í° íŒŒì¼ ì°¾ê¸° (MB ë‹¨ìœ„)"""
        size_bytes = size_mb * 1024 * 1024
        large_files = []
        
        for file_path in self.workspace.rglob("*"):
            if (file_path.is_file() and 
                file_path.stat().st_size > size_bytes and
                not str(file_path).startswith(str(self.workspace / '.git'))):
                large_files.append((file_path, file_path.stat().st_size))
        
        return sorted(large_files, key=lambda x: x[1], reverse=True)
    
    def clean_pycache(self):
        """Python ìºì‹œ íŒŒì¼ ì •ë¦¬"""
        pycache_dirs = list(self.workspace.rglob("__pycache__"))
        pyc_files = list(self.workspace.rglob("*.pyc"))
        
        for cache_dir in pycache_dirs:
            if cache_dir.is_dir():
                try:
                    shutil.rmtree(cache_dir)
                    self.cleaned_files.append(str(cache_dir))
                except:
                    pass
        
        for pyc_file in pyc_files:
            try:
                pyc_file.unlink()
                self.cleaned_files.append(str(pyc_file))
            except:
                pass
        
        return len(pycache_dirs) + len(pyc_files)
    
    def analyze_workspace(self):
        """ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ì „ì²´ ë¶„ì„"""
        analysis = {
            "temp_files": self.find_temp_files(),
            "duplicates": self.find_duplicate_files(),
            "old_logs": self.find_old_logs(),
            "empty_dirs": self.find_empty_directories(),
            "large_files": self.find_large_files(),
            "timestamp": datetime.now().isoformat()
        }
        
        return analysis
    
    def clean_safe(self, dry_run=True):
        """ì•ˆì „í•œ ì •ë¦¬ (ì„ì‹œ íŒŒì¼, ìºì‹œë§Œ)"""
        if not dry_run:
            print("ğŸ§¹ ì‹¤ì œ ì •ë¦¬ ì‹œì‘...")
        else:
            print("ğŸ” ì •ë¦¬ ì‹œë®¬ë ˆì´ì…˜ (dry run)...")
        
        # 1. Python ìºì‹œ ì •ë¦¬
        pycache_count = 0 if dry_run else self.clean_pycache()
        print(f"  Python ìºì‹œ: {len(list(self.workspace.rglob('__pycache__')))} ê°œ")
        
        # 2. ì„ì‹œ íŒŒì¼ ì •ë¦¬  
        temp_files = self.find_temp_files()
        print(f"  ì„ì‹œ íŒŒì¼: {len(temp_files)} ê°œ")
        if not dry_run:
            for temp_file in temp_files:
                try:
                    temp_file.unlink()
                    self.cleaned_files.append(str(temp_file))
                except:
                    pass
        
        # 3. ë¹ˆ ë””ë ‰í† ë¦¬ ì •ë¦¬
        empty_dirs = self.find_empty_directories()
        print(f"  ë¹ˆ ë””ë ‰í† ë¦¬: {len(empty_dirs)} ê°œ")
        if not dry_run:
            for empty_dir in empty_dirs:
                try:
                    empty_dir.rmdir()
                    self.cleaned_files.append(str(empty_dir))
                except:
                    pass
        
        # 4. 30ì¼ ì´ìƒ ëœ ë°±ì—… ì •ë¦¬
        old_backups = []
        backup_dir = self.workspace / ".agents" / "backup"
        if backup_dir.exists():
            cutoff_date = datetime.now() - timedelta(days=30)
            for backup in backup_dir.glob("backup_*.zip"):
                if datetime.fromtimestamp(backup.stat().st_mtime) < cutoff_date:
                    old_backups.append(backup)
        
        print(f"  ì˜¤ë˜ëœ ë°±ì—…: {len(old_backups)} ê°œ")
        if not dry_run:
            for old_backup in old_backups:
                try:
                    old_backup.unlink()
                    self.cleaned_files.append(str(old_backup))
                except:
                    pass
        
        total_cleaned = len(temp_files) + len(empty_dirs) + len(old_backups)
        if not dry_run:
            total_cleaned += pycache_count
        
        print(f"\n{'âœ… ì •ë¦¬ ì™„ë£Œ' if not dry_run else 'ğŸ“‹ ì •ë¦¬ ì˜ˆìƒ'}: {total_cleaned} í•­ëª©")
        
        return {
            "cleaned_count": total_cleaned,
            "temp_files": len(temp_files),
            "empty_dirs": len(empty_dirs), 
            "old_backups": len(old_backups),
            "pycache": pycache_count if not dry_run else len(list(self.workspace.rglob("__pycache__")))
        }
    
    def generate_report(self):
        """ì •ë¦¬ ë³´ê³ ì„œ ìƒì„±"""
        analysis = self.analyze_workspace()
        
        report = f"""# ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ì •ë¦¬ ë³´ê³ ì„œ
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ—‚ï¸ íŒŒì¼ ë¶„ì„

### ì„ì‹œ íŒŒì¼: {len(analysis['temp_files'])} ê°œ
{chr(10).join(f"- {f}" for f in analysis['temp_files'][:10])}
{'...' if len(analysis['temp_files']) > 10 else ''}

### ì¤‘ë³µ íŒŒì¼: {len(analysis['duplicates'])} ìŒ
{chr(10).join(f"- {dup[0]} â†” {dup[1]}" for dup in analysis['duplicates'][:5])}
{'...' if len(analysis['duplicates']) > 5 else ''}

### í° íŒŒì¼ (10MB+): {len(analysis['large_files'])} ê°œ
{chr(10).join(f"- {f[0]} ({f[1]/1024/1024:.1f}MB)" for f in analysis['large_files'][:5])}
{'...' if len(analysis['large_files']) > 5 else ''}

### ì˜¤ë˜ëœ ë¡œê·¸: {len(analysis['old_logs'])} ê°œ
{chr(10).join(f"- {f}" for f in analysis['old_logs'][:10])}
{'...' if len(analysis['old_logs']) > 10 else ''}

## ğŸ“ ë¹ˆ ë””ë ‰í† ë¦¬: {len(analysis['empty_dirs'])} ê°œ

## ğŸ’¡ ê¶Œì¥ì‚¬í•­
- ì„ì‹œ íŒŒì¼ê³¼ Python ìºì‹œëŠ” ì•ˆì „í•˜ê²Œ ì‚­ì œ ê°€ëŠ¥
- ì¤‘ë³µ íŒŒì¼ì€ ìˆ˜ë™ìœ¼ë¡œ ê²€í†  í›„ ì‚­ì œ
- í° íŒŒì¼ë“¤ì€ í•„ìš”ì„± ê²€í† 
- 30ì¼ ì´ìƒ ëœ ë¡œê·¸ëŠ” ì•„ì¹´ì´ë¸Œ ê³ ë ¤
"""
        
        return report

# CLI ì¸í„°í˜ì´ìŠ¤
if __name__ == "__main__":
    import sys
    
    cleaner = FileCleanup()
    
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python file_cleanup.py <command>")
        print("ëª…ë ¹ì–´:")
        print("  analyze     - ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ë¶„ì„")
        print("  clean       - ì•ˆì „í•œ ì •ë¦¬ ì‹¤í–‰")
        print("  clean-dry   - ì •ë¦¬ ì‹œë®¬ë ˆì´ì…˜")
        print("  report      - ìƒì„¸ ë³´ê³ ì„œ ìƒì„±")
        sys.exit(1)
    
    command = sys.argv[1]
    
    if command == "analyze":
        analysis = cleaner.analyze_workspace()
        print("ğŸ“Š ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ë¶„ì„ ê²°ê³¼:")
        print(f"  ì„ì‹œ íŒŒì¼: {len(analysis['temp_files'])} ê°œ")
        print(f"  ì¤‘ë³µ íŒŒì¼: {len(analysis['duplicates'])} ìŒ") 
        print(f"  í° íŒŒì¼: {len(analysis['large_files'])} ê°œ")
        print(f"  ë¹ˆ ë””ë ‰í† ë¦¬: {len(analysis['empty_dirs'])} ê°œ")
        print(f"  ì˜¤ë˜ëœ ë¡œê·¸: {len(analysis['old_logs'])} ê°œ")
        
    elif command == "clean-dry":
        result = cleaner.clean_safe(dry_run=True)
        
    elif command == "clean":
        result = cleaner.clean_safe(dry_run=False)
        
    elif command == "report":
        report = cleaner.generate_report()
        report_file = Path("workspace_cleanup_report.md")
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"ğŸ“„ ë³´ê³ ì„œ ìƒì„±: {report_file}")
        
    else:
        print(f"ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´: {command}")
        sys.exit(1)